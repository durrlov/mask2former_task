{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPceH8gKbvbdnVzelygl5wQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKc1UqZmXi4x","executionInfo":{"status":"ok","timestamp":1761311904840,"user_tz":-360,"elapsed":21732,"user":{"displayName":"durlov","userId":"13867110916857444773"}},"outputId":"3f1f6f31-62f1-49c3-e15e-1ad5a0e7ff18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install --pre 'git+https://github.com/facebookresearch/detectron2.git'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BP_0S5l5XjeR","executionInfo":{"status":"ok","timestamp":1761312060661,"user_tz":-360,"elapsed":152155,"user":{"displayName":"durlov","userId":"13867110916857444773"}},"outputId":"423ba573-76e2-45c5-d86d-9f540cc3dae9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/detectron2.git\n","  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-blsdgznb\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-blsdgznb\n","  Resolved https://github.com/facebookresearch/detectron2.git to commit a1ce2f956a1d2212ad672e3c47d53405c2fe4312\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.0)\n","Collecting yacs>=0.1.8 (from detectron2==0.6)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.1)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n","Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n","  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n","Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n","Collecting hydra-core>=1.1 (from detectron2==0.6)\n","  Downloading hydra_core-1.4.0.dev1-py3-none-any.whl.metadata (5.4 kB)\n","Collecting black (from detectron2==0.6)\n","  Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n","INFO: pip is looking at multiple versions of hydra-core to determine which version is compatible with other requirements. This could take a while.\n","Collecting hydra-core>=1.1 (from detectron2==0.6)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n","Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.0)\n","Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n","  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.0)\n","Collecting pytokens>=0.1.10 (from black->detectron2==0.6)\n","  Downloading pytokens-0.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.75.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.9)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n","Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n","Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Downloading pytokens-0.2.0-py3-none-any.whl (12 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Building wheels for collected packages: detectron2, fvcore\n","  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=6352512 sha256=a3ac7c8aec316dee4d545730dc62b1d7bd0305ba034cec6815304d9ce66e6d64\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-13xsha16/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=158493812b8a2944186c55ecfc91e38d4f8dd3118530a8ba7bbd5ad93e39f2d2\n","  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n","Successfully built detectron2 fvcore\n","Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n","Successfully installed black-25.9.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 pytokens-0.2.0 yacs-0.1.8\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1R3bmcSeZiAB3MJgwr2N0lrR69mK4XWQo"},"id":"Px-9hk7PXWRl","executionInfo":{"status":"ok","timestamp":1761312134421,"user_tz":-360,"elapsed":29879,"user":{"displayName":"durlov","userId":"13867110916857444773"}},"outputId":"e03a5e7e-df32-47a8-8e3d-2cf44d839d52"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Tree Instance Segmentation - Dataset Registration for Detectron2\n","\n","import os\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","from detectron2.data.datasets import register_coco_instances\n","\n","# Configuration\n","\n","COCO_JSON_DIR = \"/content/drive/MyDrive/mask2former_task/coco_format\"\n","IMAGE_ROOT = \"/content/drive/MyDrive/mask2former_task/preprocessed_data\"\n","\n","TRAIN_DATASET_NAME = \"tree_train\"\n","VAL_DATASET_NAME = \"tree_val\"\n","\n","# Registration Function\n","def register_tree_dataset():\n","    \"\"\"\n","    Register the tree instance segmentation dataset with Detectron2.\n","\n","    This function:\n","    1. Registers both train and val splits\n","    2. Sets up metadata (class names, colors)\n","    3. Verifies the registration was successful\n","    \"\"\"\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"REGISTERING TREE DATASET WITH DETECTRON2\")\n","    print(\"=\"*70 + \"\\n\")\n","\n","    # Register Training Set\n","    train_json = os.path.join(COCO_JSON_DIR, \"instances_train.json\")\n","    train_images = os.path.join(IMAGE_ROOT, \"train_PNG\")\n","\n","    print(f\"📝 Registering training set: '{TRAIN_DATASET_NAME}'\")\n","    print(f\"   JSON: {train_json}\")\n","    print(f\"   Images: {train_images}\")\n","\n","    if not os.path.exists(train_json):\n","        raise FileNotFoundError(f\"Training JSON not found: {train_json}\")\n","    if not os.path.exists(train_images):\n","        raise FileNotFoundError(f\"Training images directory not found: {train_images}\")\n","\n","    register_coco_instances(\n","        name=TRAIN_DATASET_NAME,\n","        metadata={},\n","        json_file=train_json,\n","        image_root=train_images\n","    )\n","    print(\"   ✓ Training set registered successfully\\n\")\n","\n","    # Register Validation Set\n","    val_json = os.path.join(COCO_JSON_DIR, \"instances_val.json\")\n","    val_images = os.path.join(IMAGE_ROOT, \"val_PNG\")\n","\n","    print(f\"📝 Registering validation set: '{VAL_DATASET_NAME}'\")\n","    print(f\"   JSON: {val_json}\")\n","    print(f\"   Images: {val_images}\")\n","\n","    if not os.path.exists(val_json):\n","        raise FileNotFoundError(f\"Validation JSON not found: {val_json}\")\n","    if not os.path.exists(val_images):\n","        raise FileNotFoundError(f\"Validation images directory not found: {val_images}\")\n","\n","    register_coco_instances(\n","        name=VAL_DATASET_NAME,\n","        metadata={},\n","        json_file=val_json,\n","        image_root=val_images\n","    )\n","    print(\"Validation set registered successfully\\n\")\n","\n","    # Set Metadata\n","    print(\" Setting dataset metadata...\")\n","\n","    metadata = {\n","        \"thing_classes\": [\"tree\"],\n","        \"thing_colors\": [(34, 139, 34)],\n","    }\n","\n","    # Apply metadata to both datasets\n","    MetadataCatalog.get(TRAIN_DATASET_NAME).set(**metadata)\n","    MetadataCatalog.get(VAL_DATASET_NAME).set(**metadata)\n","\n","    print(\"Metadata configured\")\n","    print(f\"      - Classes: {metadata['thing_classes']}\")\n","    print(f\"      - Colors: {metadata['thing_colors']}\\n\")\n","\n","    # Verify Registration\n","    print(\"🔍 Verifying registration...\")\n","\n","    # Check training dataset\n","    train_dicts = DatasetCatalog.get(TRAIN_DATASET_NAME)\n","    train_metadata = MetadataCatalog.get(TRAIN_DATASET_NAME)\n","    print(f\"   ✓ Training: {len(train_dicts)} images loaded\")\n","    print(f\"      - Classes: {train_metadata.thing_classes}\")\n","\n","    # Check validation dataset\n","    val_dicts = DatasetCatalog.get(VAL_DATASET_NAME)\n","    val_metadata = MetadataCatalog.get(VAL_DATASET_NAME)\n","    print(f\"   ✓ Validation: {len(val_dicts)} images loaded\")\n","    print(f\"      - Classes: {val_metadata.thing_classes}\\n\")\n","\n","    # Sample Data Inspection\n","    print(\"Sample data inspection...\")\n","\n","    sample = train_dicts[0]\n","    print(f\"\\n   Sample image: {sample['file_name']}\")\n","    print(f\"   - Image ID: {sample['image_id']}\")\n","    print(f\"   - Dimensions: {sample['height']}x{sample['width']}\")\n","    print(f\"   - Annotations: {len(sample['annotations'])} instances\")\n","\n","    if len(sample['annotations']) > 0:\n","        ann = sample['annotations'][0]\n","        print(f\"\\n   Sample annotation:\")\n","        print(f\"   - Category ID: {ann['category_id']}\")\n","        print(f\"   - Bbox: {ann['bbox']}\")\n","        print(f\"   - Area: {ann.get('area', 'N/A')}\")\n","        print(f\"   - Segmentation type: {type(ann['segmentation']).__name__}\")\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"DATASET REGISTRATION COMPLETE\")\n","    print(\"=\"*70)\n","    print(f\"\\nDataset names for training config:\")\n","    print(f\"   - TRAIN: '{TRAIN_DATASET_NAME}'\")\n","    print(f\"   - VAL:   '{VAL_DATASET_NAME}'\")\n","    print(\"=\"*70 + \"\\n\")\n","\n","    return TRAIN_DATASET_NAME, VAL_DATASET_NAME\n","\n","\n","# VISUALIZATION FUNCTION\n","\n","def visualize_registered_dataset(dataset_name, num_samples=3):\n","    \"\"\"\n","    Visualize some samples from the registered dataset.\n","\n","    \"\"\"\n","    import cv2\n","    import random\n","    import matplotlib.pyplot as plt\n","    from detectron2.utils.visualizer import Visualizer\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"VISUALIZING DATASET: {dataset_name}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    dataset_dicts = DatasetCatalog.get(dataset_name)\n","    metadata = MetadataCatalog.get(dataset_name)\n","\n","    samples = random.sample(dataset_dicts, min(num_samples, len(dataset_dicts)))\n","\n","    fig, axes = plt.subplots(1, len(samples), figsize=(6*len(samples), 6))\n","    if len(samples) == 1:\n","        axes = [axes]\n","\n","    for idx, (sample, ax) in enumerate(zip(samples, axes)):\n","        img = cv2.imread(sample[\"file_name\"])\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        visualizer = Visualizer(img, metadata=metadata, scale=1.0)\n","        vis = visualizer.draw_dataset_dict(sample)\n","\n","        ax.imshow(vis.get_image())\n","        ax.set_title(f\"Image {idx+1}\\n{len(sample['annotations'])} instances\",\n","                     fontsize=12)\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","\n","    output_path = os.path.join(COCO_JSON_DIR, f\"{dataset_name}_samples.png\")\n","    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(f\"✓ Visualization saved to: {output_path}\\n\")\n","\n","\n","\n","def print_dataset_statistics(dataset_name):\n","    \"\"\"\n","    Print detailed statistics about the registered dataset.\n","\n","    \"\"\"\n","    import numpy as np\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"DATASET STATISTICS: {dataset_name}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    dataset_dicts = DatasetCatalog.get(dataset_name)\n","    metadata = MetadataCatalog.get(dataset_name)\n","\n","    total_images = len(dataset_dicts)\n","    total_annotations = sum(len(d['annotations']) for d in dataset_dicts)\n","\n","    instances_per_image = [len(d['annotations']) for d in dataset_dicts]\n","\n","    all_areas = []\n","    for d in dataset_dicts:\n","        for ann in d['annotations']:\n","            all_areas.append(ann.get('area', 0))\n","\n","    print(f\" Overall Statistics:\")\n","    print(f\"   - Total images: {total_images}\")\n","    print(f\"   - Total instances: {total_annotations}\")\n","    print(f\"   - Classes: {metadata.thing_classes}\")\n","\n","    print(f\"\\n Instance Distribution:\")\n","    print(f\"   - Mean per image: {np.mean(instances_per_image):.2f}\")\n","    print(f\"   - Median per image: {np.median(instances_per_image):.0f}\")\n","    print(f\"   - Min per image: {np.min(instances_per_image)}\")\n","    print(f\"   - Max per image: {np.max(instances_per_image)}\")\n","\n","    print(f\"\\n Instance Area (pixels):\")\n","    print(f\"   - Mean: {np.mean(all_areas):.2f}\")\n","    print(f\"   - Median: {np.median(all_areas):.2f}\")\n","    print(f\"   - Min: {np.min(all_areas):.2f}\")\n","    print(f\"   - Max: {np.max(all_areas):.2f}\")\n","\n","    print(f\"\\n{'='*70}\\n\")\n","\n","\n","def main():\n","    \"\"\"\n","    Main function to register and verify the dataset.\n","    \"\"\"\n","    try:\n","        # Register dataset\n","        train_name, val_name = register_tree_dataset()\n","\n","        print_dataset_statistics(train_name)\n","        print_dataset_statistics(val_name)\n","\n","        print(\"Creating visualizations...\")\n","        visualize_registered_dataset(train_name, num_samples=3)\n","        visualize_registered_dataset(val_name, num_samples=3)\n","\n","        print(\"\\n\" + \"=\"*70)\n","        print(\" SUCCESS! Dataset is ready for training\")\n","        print(\"=\"*70)\n","        print(\"\\n Use these dataset names in your training config:\")\n","        print(f\"   cfg.DATASETS.TRAIN = ('{train_name}',)\")\n","        print(f\"   cfg.DATASETS.TEST = ('{val_name}',)\")\n","        print(\"=\"*70 + \"\\n\")\n","\n","    except Exception as e:\n","        print(f\"\\n Error during registration: {str(e)}\")\n","        raise\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"A8K-PM_4Xbyi"},"execution_count":null,"outputs":[]}]}